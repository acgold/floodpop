{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download FEMA floodplain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ modified from https://github.com/jkrohn5/Code-Snips/blob/main/downloadfloodhazard.py #################\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import io\n",
    "import zipfile\n",
    "import arcpy\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import urllib.request\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "arcpy.env.outputCoordinateSystem = arcpy.SpatialReference(\"USA Contiguous Albers Equal Area Conic USGS\")\n",
    "arcpy.env.parallelProcessingFactor = \"100%\"\n",
    "\n",
    "\n",
    "data_path = \"your_path_here\"\n",
    "\n",
    "##Change this to working directory\n",
    "os.chdir(data_path)\n",
    "base_url = 'https://hazards.fema.gov/nfhlv2/output/State/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping Census state FIPS codes to state abbreviations\n",
    "fips_to_abbreviation = {\n",
    "    '01': 'AL',  # Alabama\n",
    "    '02': 'AK',  # Alaska\n",
    "    '04': 'AZ',  # Arizona: No state layer available\n",
    "    '05': 'AR',  # Arkansas\n",
    "    '06': 'CA',  # California\n",
    "    '08': 'CO',  # Colorado: No state layer available\n",
    "    '09': 'CT',  # Connecticut\n",
    "    '10': 'DE',  # Delaware\n",
    "    '11': 'DC',  # District of Columbia\n",
    "    '12': 'FL',  # Florida: No state layer available\n",
    "    '13': 'GA',  # Georgia\n",
    "    '15': 'HI',  # Hawaii\n",
    "    '16': 'ID',  # Idaho\n",
    "    '17': 'IL',  # Illinois: No state layer available\n",
    "    '18': 'IN',  # Indiana\n",
    "    '19': 'IA',  # Iowa\n",
    "    '20': 'KS',  # Kansas\n",
    "    '21': 'KY',  # Kentucky\n",
    "    '22': 'LA',  # Louisiana\n",
    "    '23': 'ME',  # Maine\n",
    "    '24': 'MD',  # Maryland\n",
    "    '25': 'MA',  # Massachusetts\n",
    "    '26': 'MI',  # Michigan\n",
    "    '27': 'MN',  # Minnesota\n",
    "    '28': 'MS',  # Mississippi: No state layer available\n",
    "    '29': 'MO',  # Missouri\n",
    "    '30': 'MT',  # Montana\n",
    "    '31': 'NE',  # Nebraska\n",
    "    '32': 'NV',  # Nevada\n",
    "    '33': 'NH',  # New Hampshire\n",
    "    '34': 'NJ',  # New Jersey\n",
    "    '35': 'NM',  # New Mexico: No state layer available\n",
    "    '36': 'NY',  # New York\n",
    "    '37': 'NC',  # North Carolina\n",
    "    '38': 'ND',  # North Dakota\n",
    "    '39': 'OH',  # Ohio\n",
    "    '40': 'OK',  # Oklahoma\n",
    "    '41': 'OR',  # Oregon\n",
    "    '42': 'PA',  # Pennsylvania\n",
    "    '44': 'RI',  # Rhode Island\n",
    "    '45': 'SC',  # South Carolina\n",
    "    '46': 'SD',  # South Dakota\n",
    "    '47': 'TN',  # Tennessee\n",
    "    '48': 'TX',  # Texas: No state layer available\n",
    "    '49': 'UT',  # Utah\n",
    "    '50': 'VT',  # Vermont\n",
    "    '51': 'VA',  # Virginia\n",
    "    '53': 'WA',  # Washington\n",
    "    '54': 'WV',  # West Virginia\n",
    "    '55': 'WI',  # Wisconsin\n",
    "    '56': 'WY'   # Wyoming\n",
    "}\n",
    "\n",
    "stfips = list(fips_to_abbreviation.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape HTML to get download links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get HTML from NFHL database\n",
    "url = 'https://hazards.fema.gov/femaportal/NFHL/searchResult'\n",
    "output_file = 'nfhl.html'\n",
    "\n",
    "try:\n",
    "    response = requests.get(url, timeout=120)  # Set timeout to 120 seconds\n",
    "    response.raise_for_status()  # Check if the request was successful (status code 200)\n",
    "    \n",
    "    with open(output_file, 'w') as file:\n",
    "        file.write(response.text)  # Save the content to a file\n",
    "    \n",
    "    print(f\"Downloaded the file successfully: {output_file}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe of download links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML table into a Pandas dataframe, then save as csv\n",
    "# Define file paths\n",
    "input_file = 'nfhl.html'\n",
    "output_file = 'parsed_nfhl_table.csv'\n",
    "\n",
    "# Load the HTML file\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "# Find the first table\n",
    "table = soup.find('table')\n",
    "\n",
    "# Extract table headers and format them\n",
    "headers = [header.text.strip().lower().replace(' ', '_') for header in table.find_all('th')]\n",
    "\n",
    "# Extract table rows\n",
    "rows = []\n",
    "for row in table.find('tbody').find_all('tr'):\n",
    "    cols = [col.text.strip() for col in row.find_all('td')]\n",
    "\n",
    "    # Clean the \"county\" column to remove duplicate names and extra whitespace\n",
    "    if len(cols) > 1:\n",
    "        county = cols[1]\n",
    "        county = county.split(\"\\n\")[0].strip()\n",
    "        cols[1] = county\n",
    "\n",
    "    # Parse the \"update_date\" column and reformat the date\n",
    "    if len(cols) > 3:\n",
    "        raw_date = cols[3]\n",
    "        \n",
    "        try:\n",
    "            # Extract the first occurrence of the date\n",
    "            cleaned_date = raw_date.split('\\n')[0].strip()\n",
    "            split_date = cleaned_date.split()\n",
    "            del split_date[-2]\n",
    "            cleaned_date_without_tz = \" \".join(split_date)\n",
    "            \n",
    "            parsed_date = datetime.strptime(cleaned_date_without_tz, \"%a %b %d %H:%M:%S %Y\")\n",
    "\n",
    "            formatted_date = parsed_date.strftime(\"%m/%d/%Y\")\n",
    "            cols[3] = formatted_date\n",
    "        except ValueError:\n",
    "            # If date parsing fails, keep the original text\n",
    "            pass\n",
    "\n",
    "    # Get download URL, fix it, and prepend the base URL\n",
    "    download_col = row.find('a')['href']\n",
    "    download_url = \"https://hazards.fema.gov/femaportal/NFHL/\" + download_col.replace(' ', '%20')\n",
    "    cols.append(download_url)\n",
    "\n",
    "    rows.append(cols)\n",
    "\n",
    "# Add \"download_url\" to headers\n",
    "headers.append(\"download_url\")\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(rows, columns=headers)\n",
    "df = df.drop('download', axis=1)\n",
    "\n",
    "# Dictionary mapping full state names to abbreviations\n",
    "state_abbreviations = {\n",
    "    'ALABAMA': 'AL', 'ALASKA': 'AK', 'ARIZONA': 'AZ', 'ARKANSAS': 'AR', 'CALIFORNIA': 'CA',\n",
    "    'COLORADO': 'CO', 'CONNECTICUT': 'CT', 'DISTRICT OF COLUMBIA': 'DC', 'DELAWARE': 'DE', 'FLORIDA': 'FL', 'GEORGIA': 'GA',\n",
    "    'HAWAII': 'HI', 'IDAHO': 'ID', 'ILLINOIS': 'IL', 'INDIANA': 'IN', 'IOWA': 'IA',\n",
    "    'KANSAS': 'KS', 'KENTUCKY': 'KY', 'LOUISIANA': 'LA', 'MAINE': 'ME', 'MARYLAND': 'MD',\n",
    "    'MASSACHUSETTS': 'MA', 'MICHIGAN': 'MI', 'MINNESOTA': 'MN', 'MISSISSIPPI': 'MS', 'MISSOURI': 'MO',\n",
    "    'MONTANA': 'MT', 'NEBRASKA': 'NE', 'NEVADA': 'NV', 'NEW HAMPSHIRE': 'NH', 'NEW JERSEY': 'NJ',\n",
    "    'NEW MEXICO': 'NM', 'NEW YORK': 'NY', 'NORTH CAROLINA': 'NC', 'NORTH DAKOTA': 'ND', 'OHIO': 'OH',\n",
    "    'OKLAHOMA': 'OK', 'OREGON': 'OR', 'PENNSYLVANIA': 'PA', 'RHODE ISLAND': 'RI', 'SOUTH CAROLINA': 'SC',\n",
    "    'SOUTH DAKOTA': 'SD', 'TENNESSEE': 'TN', 'TEXAS': 'TX', 'UTAH': 'UT', 'VERMONT': 'VT',\n",
    "    'VIRGINIA': 'VA', 'WASHINGTON': 'WA', 'WEST VIRGINIA': 'WV', 'WISCONSIN': 'WI', 'WYOMING': 'WY',\n",
    "    'PUERTO RICO': 'PR', 'VIRGIN ISLANDS': 'VI', 'GUAM': 'GU', 'AMERICAN SAMOA': 'AS', 'NORTHERN MARIANA ISLANDS': 'MP'\n",
    "}\n",
    "\n",
    "# Function to replace full state name with abbreviation\n",
    "df['state_abb'] = df['state'].apply(lambda x: state_abbreviations.get(x, x))  # Use the abbreviation if it exists, else keep the original\n",
    "\n",
    "# Save to a CSV file\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"DataFrame saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data for every county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('parsed_nfhl_table.csv')\n",
    "\n",
    "# Location to save the files\n",
    "location = \"nfhl_zipped\"\n",
    "\n",
    "nrows = len(df)\n",
    "\n",
    "# DataFrame to store error logs\n",
    "error_log = pd.DataFrame(columns=['index', 'url', 'item_id', 'error_message'])\n",
    "\n",
    "# Loop through the states and download the files\n",
    "for index, row in df.iterrows():\n",
    "    # print(index)\n",
    "    # Construct the URL for the download\n",
    "    url = row['download_url']\n",
    "    st_abb = row['state_abb']\n",
    "    item_id = row['item_id'].replace('-NFHL', '')\n",
    "    date = row['update_date'].replace('/', '')\n",
    "    \n",
    "    # Construct the file path to save the downloaded file\n",
    "    file_path = os.path.join(location, f\"{st_abb}_{item_id}_{date}.zip\")\n",
    "    print(f\"({index+1}/{nrows}) \", row['size'], file_path)\n",
    "    \n",
    "    try:\n",
    "        # Download the file\n",
    "        urllib.request.urlretrieve(url, file_path)\n",
    "    except Exception as e:\n",
    "        # If there's an error, print it and continue\n",
    "        print(f\"Failed to download {url}: {e}\")\n",
    "\n",
    "        # Log the error details in the error_log DataFrame\n",
    "        error_log = error_log.append({\n",
    "            'index': index,\n",
    "            'url': url,\n",
    "            'item_id': item_id,\n",
    "            'error_message': str(e)\n",
    "        }, ignore_index=True)\n",
    "\n",
    "    time.sleep(0.5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Unzip the nfhl\n",
    "zipped_folder = \"nfhl_zipped\"\n",
    "unzipped_folder = \"nfhl_unzipped\"\n",
    "os.makedirs(unzipped_folder, exist_ok=True)\n",
    "\n",
    "file_prefix = \"S_FLD_HAZ_AR\"\n",
    "\n",
    "# Loop through each ZIP file in the folder\n",
    "for zip_filename in os.listdir(zipped_folder):\n",
    "    if zip_filename.endswith(\".zip\"):\n",
    "        zip_path = os.path.join(zipped_folder, zip_filename)\n",
    "        \n",
    "        # Create a new folder to extract to (named after the zip file without the extension)\n",
    "        extract_folder = os.path.join(unzipped_folder, os.path.splitext(zip_filename)[0])\n",
    "        os.makedirs(extract_folder, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            # Open the ZIP file\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                # Get all files in the ZIP archive\n",
    "                files_in_zip = zip_ref.namelist()\n",
    "                \n",
    "                # Loop through the files and extract only those with the desired prefix\n",
    "                for file_name in files_in_zip:\n",
    "                    if file_name.startswith(file_prefix):  # Check if the file starts with the prefix\n",
    "                        # Extract the file to the designated folder\n",
    "                        zip_ref.extract(file_name, extract_folder)\n",
    "                        print(f\"Extracted: {file_name} from {zip_filename}\")\n",
    "            \n",
    "            print(f\"Successfully extracted files from: {zip_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract {zip_filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the necessary GDBs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('parsed_nfhl_table.csv')\n",
    "\n",
    "# Set the output geodatabase path\n",
    "input_folder = \"nfhl_unzipped\"\n",
    "output_nfhl_gdb = \"nfhl_dec_12_2024.gdb\"\n",
    "output_sfha_gdb = \"sfha_dec_12_2024.gdb\"\n",
    "\n",
    "# Check if the geodatabases exists\n",
    "if not arcpy.Exists(output_nfhl_gdb):\n",
    "    # If it doesn't exist, create the geodatabase\n",
    "    arcpy.CreateFileGDB_management(os.path.dirname(output_nfhl_gdb), os.path.basename(output_nfhl_gdb))\n",
    "    print(f\"Geodatabase created: {output_nfhl_gdb}\")\n",
    "else:\n",
    "    print(f\"Geodatabase already exists: {output_nfhl_gdb}\")\n",
    "\n",
    "\n",
    "if not arcpy.Exists(output_sfha_gdb):\n",
    "    # If it doesn't exist, create the geodatabase\n",
    "    arcpy.CreateFileGDB_management(os.path.dirname(output_sfha_gdb), os.path.basename(output_sfha_gdb))\n",
    "    print(f\"Geodatabase created: {output_sfha_gdb}\")\n",
    "else:\n",
    "    print(f\"Geodatabase already exists: {output_sfha_gdb}\")\n",
    "    \n",
    "# Iterate through each state and find the folders for each state. Then merge them, extract SFHA and repair\n",
    "state_abb = sorted(df.state_abb.unique())\n",
    "state_abb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create NFHL and SFHA feature classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_list = os.listdir(input_folder)\n",
    "\n",
    "for st in state_abb:\n",
    "    print(f\"Working on: {st}\")\n",
    "    state_layers = []\n",
    "\n",
    "    for folder_name in folder_list:\n",
    "        if folder_name.split('_')[0] == st:\n",
    "            shp_path = os.path.join(data_path, \"nfhl_unzipped\", folder_name, \"S_FLD_HAZ_AR.shp\")\n",
    "            if os.path.exists(shp_path):\n",
    "                state_layers.append(shp_path)\n",
    "                # print(f\"Added {shp_path} to state list\")\n",
    "            else:\n",
    "                print(f\"File does not exists at: {shp_path}\")\n",
    "\n",
    "    print(f\"Merging {len(state_layers)} layers of NFHL for {st}\")\n",
    "    arcpy.management.Merge(state_layers, os.path.join(output_nfhl_gdb, st + \"_nfhl\"))\n",
    "\n",
    "    print(f\"Selecting and repairing the SFHA for {st}\")\n",
    "    arcpy.analysis.Select(\n",
    "            in_features=os.path.join(output_nfhl_gdb, st + \"_nfhl\"),\n",
    "            out_feature_class=os.path.join(output_sfha_gdb, st + \"_sfha\"),\n",
    "            where_clause=\"SFHA_TF = 'T'\"\n",
    "        )\n",
    "\n",
    "    arcpy.management.RepairGeometry(os.path.join(output_sfha_gdb, st + \"_sfha\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
