{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate Census data confidence intervals\n",
    "See [this website](https://registry.opendata.aws/census-2020-amc-mdf-replicates/?utm_campaign=20241024cnmps1&utm_medium=email&utm_source=govdelivery) for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the replicates data \n",
    "This runs in the terminal. Check the linked website above for specific download methods. Replace \"your-path\" in the code.\n",
    "\n",
    "`s3 cp --no-sign-request --recursive s3://uscb-2020-product-releases/decennial/amc/2020/mdf/2020-dhc-mdf-replicates/ppmf_gzip/ \"your_path\\PPMF\\gz_csv\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a table for the baseline scenario (de-identified individual responses that make up reported 2020 Census data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import os\n",
    "\n",
    "data_path = \"your_path\"\n",
    "\n",
    "# Specify the CSV file path\n",
    "csv_path = os.path.join(data_path, 'gz_csvs')\n",
    "\n",
    "# Connect to DuckDB\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Step: Drop the temporary table if it exists\n",
    "drop_query = f\"DROP TABLE IF EXISTS baseline_table\"\n",
    "\n",
    "# Execute the query to drop the table\n",
    "con.execute(drop_query)\n",
    "\n",
    "# Query to count rows for unique combinations of the four columns\n",
    "initial_query = f\"\"\"\n",
    "CREATE TABLE baseline_table AS\n",
    "SELECT \n",
    "    CONCAT(TABBLKST, TABBLKCOU, TABTRACTCE, TABBLK) AS GEOID20,\n",
    "    COUNT(*) AS PPMF0_COUNT \n",
    "FROM read_csv_auto('{os.path.join(csv_path,\"PPMF20_0_PER.csv.gz\")}')\n",
    "GROUP BY TABBLKST, TABBLKCOU, TABTRACTCE, TABBLK\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "con.execute(initial_query)\n",
    "print(\"Created baseline table using PPMF0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through each supplied iteration file and calculat the difference from baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define a function to process each iteration file, calculate COUNT, and calculate the differences\n",
    "def process_iteration_file(iteration_file, iteration_number):\n",
    "    print(iteration_file)\n",
    "    \n",
    "    # Step: Drop the temporary table if it exists\n",
    "    drop_query = f\"DROP TABLE IF EXISTS iteration_{iteration_number}\"\n",
    "\n",
    "    # Execute the query to drop the table\n",
    "    con.execute(drop_query)\n",
    "    \n",
    "    # Step 2a: Read the iteration file and calculate COUNT, then join it with the baseline_table\n",
    "    query = f\"\"\"\n",
    "    CREATE TEMPORARY TABLE iteration_{iteration_number} AS\n",
    "    SELECT \n",
    "        CONCAT(b.TABBLKST, b.TABBLKCOU, b.TABTRACTCE, b.TABBLK) AS GEOID20,\n",
    "        COALESCE(a.PPMF0_COUNT, 0) AS PPMF0_COUNT,\n",
    "        COUNT(*) AS COUNT,\n",
    "        {iteration_number} as ITERATION,\n",
    "        (COUNT(*) - COALESCE(a.PPMF0_COUNT, 0)) AS DIFF,\n",
    "        ABS(COUNT(*) - COALESCE(a.PPMF0_COUNT, 0)) AS ABS_DIFF,\n",
    "        POW((COUNT(*) - COALESCE(a.PPMF0_COUNT, 0)), 2) AS SQRD_ERROR\n",
    "    FROM read_csv_auto('{iteration_file}') b\n",
    "    LEFT JOIN baseline_table a\n",
    "    ON a.GEOID20 = CONCAT(b.TABBLKST, b.TABBLKCOU, b.TABTRACTCE, b.TABBLK) \n",
    "    GROUP BY b.TABBLKST, b.TABBLKCOU, b.TABTRACTCE, b.TABBLK, a.PPMF0_COUNT, ITERATION\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the join and calculation query\n",
    "    con.execute(query)\n",
    "\n",
    "    # Step 2b: Insert the calculated results into the final result table\n",
    "    insert_query = f\"\"\"\n",
    "    INSERT INTO result_table (GEOID20, PPMF0_COUNT, ITERATION, COUNT, DIFF, ABS_DIFF, SQRD_ERROR)\n",
    "    SELECT GEOID20, PPMF0_COUNT, ITERATION, COUNT, DIFF, ABS_DIFF, SQRD_ERROR FROM iteration_{iteration_number}\n",
    "    \"\"\"\n",
    "    \n",
    "    con.execute(insert_query)\n",
    "\n",
    "    # Step: Drop the temporary table if it exists\n",
    "    drop_query = f\"DROP TABLE IF EXISTS iteration_{iteration_number}\"\n",
    "\n",
    "    # Execute the query to drop the table\n",
    "    con.execute(drop_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create a final result table to store all results\n",
    "con.execute(\"DROP TABLE IF EXISTS result_table\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE result_table (\n",
    "    GEOID20 VARCHAR, \n",
    "    PPMF0_COUNT BIGINT, \n",
    "    ITERATION INT, \n",
    "    COUNT BIGINT, \n",
    "    DIFF BIGINT, \n",
    "    ABS_DIFF BIGINT, \n",
    "    SQRD_ERROR DOUBLE\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Loop through the iteration files\n",
    "gz_csvs_folder = os.path.join(data_path, \"gz_csvs\")\n",
    "\n",
    "iteration_files = [os.path.join(gz_csvs_folder, f\"PPMF20_{i}_PER.csv.gz\") for i in range(1,51)]\n",
    "\n",
    "# Process each iteration file and calculate differences\n",
    "for i, iteration_file in enumerate(iteration_files, start=1):\n",
    "    process_iteration_file(iteration_file, i)\n",
    "\n",
    "# Step 5: Retrieve the final result (optional)\n",
    "# con.execute(\"SELECT count(*) FROM result_table\").fetchall()\n",
    "# final_result = con.execute(\"SELECT * FROM result_table\").fetchdf()\n",
    "# print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate stats on all iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step: Group by GEOID20 and calculate the statistics (MEAN, MSE, STD)\n",
    "query = \"\"\"\n",
    "CREATE TABLE summary_stats AS\n",
    "SELECT \n",
    "    GEOID20,\n",
    "    PPMF0_COUNT,\n",
    "    AVG(COUNT) AS MEAN,\n",
    "    AVG(SQRD_ERROR) AS MSE,\n",
    "    STDDEV(COUNT) AS STD\n",
    "FROM result_table  \n",
    "GROUP BY GEOID20, PPMF0_COUNT;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query to create the summary statistics table\n",
    "con.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "ci_level = 0.9\n",
    "degrees_freedom = 5\n",
    "\n",
    "# Step 1: Set the t-value\n",
    "t_value = scipy.stats.t.ppf(q=1-(1-ci_level)/2, df=degrees_freedom) \n",
    "t_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Add intermediate calculations\n",
    "con.execute(\"DROP TABLE IF EXISTS intermediate\")\n",
    "\n",
    "intermediate_query = f\"\"\"\n",
    "CREATE TEMPORARY TABLE intermediate AS\n",
    "SELECT \n",
    "    *,\n",
    "    POWER(MSE, 0.5) AS RMSE,  \n",
    "    (MEAN - PPMF0_COUNT) AS BIAS,  \n",
    "    CASE \n",
    "        WHEN PPMF0_COUNT > 5 \n",
    "             AND ABS(MEAN - PPMF0_COUNT) / NULLIF(STD, 0) >= 0.5 \n",
    "             AND ((MEAN - PPMF0_COUNT) < 0 OR PPMF0_COUNT >= 25)\n",
    "        THEN TRUE \n",
    "        ELSE FALSE \n",
    "    END AS MET_CRITERIA, \n",
    "    CASE \n",
    "        WHEN PPMF0_COUNT > 5 \n",
    "             AND ABS(MEAN - PPMF0_COUNT) / NULLIF(STD, 0) >= 0.5 \n",
    "             AND ((MEAN - PPMF0_COUNT) < 0 OR PPMF0_COUNT >= 25)\n",
    "        THEN PPMF0_COUNT - (MEAN - PPMF0_COUNT) \n",
    "        ELSE PPMF0_COUNT \n",
    "    END AS POINT_EST\n",
    "FROM summary_stats;\n",
    "\"\"\"\n",
    "con.execute(intermediate_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Calculate confidence intervals and create the final table\n",
    "con.execute(\"DROP TABLE IF EXISTS result_with_ci\")\n",
    "\n",
    "final_query = f\"\"\"\n",
    "CREATE TABLE result_with_ci AS\n",
    "SELECT \n",
    "    *,\n",
    "    GREATEST(FLOOR(POINT_EST - {t_value} * RMSE), 0) AS CI_LOW, \n",
    "    CEIL(POINT_EST + {t_value} * RMSE) AS CI_HIGH\n",
    "FROM intermediate;\n",
    "\"\"\"\n",
    "con.execute(final_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results_file = os.path.join(data_path, \"parquet/final_results.parquet\")\n",
    "\n",
    "# Step 2: Write the table to a Parquet file\n",
    "con.execute(f\"\"\"\n",
    "    COPY result_with_ci TO '{final_results_file}' (FORMAT PARQUET)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = os.path.join(data_path, \"parquet/result_table.parquet\")\n",
    "\n",
    "# Step 2: Write the table to a Parquet file\n",
    "con.execute(f\"\"\"\n",
    "    COPY result_table TO '{result_file}' (FORMAT PARQUET)\n",
    "\"\"\")\n",
    "\n",
    "baseline_file = os.path.join(data_path, \"parquet/baseline_table.parquet\")\n",
    "\n",
    "# Step 2: Write the table to a Parquet file\n",
    "con.execute(f\"\"\"\n",
    "    COPY baseline_table TO '{baseline_file}' (FORMAT PARQUET)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats_file = os.path.join(data_path, \"parquet/summary_stats_table.parquet\")\n",
    "\n",
    "# Step 2: Write the table to a Parquet file\n",
    "con.execute(f\"\"\"\n",
    "    COPY summary_stats TO '{summary_stats_file}' (FORMAT PARQUET)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat process with housing units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Housing Units Now\n",
    "# Specify the CSV file path\n",
    "csv_path = os.path.join(data_path, 'gz_csvs')\n",
    "\n",
    "# Connect to DuckDB\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Step: Drop the temporary table if it exists\n",
    "drop_query = f\"DROP TABLE IF EXISTS baseline_table_hu\"\n",
    "\n",
    "# Execute the query to drop the table\n",
    "con.execute(drop_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to count rows for unique combinations of the four columns\n",
    "initial_query = f\"\"\"\n",
    "CREATE TABLE baseline_table_hu AS\n",
    "SELECT \n",
    "    CONCAT(TABBLKST, TABBLKCOU, TABTRACTCE, TABBLK) AS GEOID20,\n",
    "    COUNT(*) AS PPMF0_COUNT \n",
    "FROM read_csv_auto('{os.path.join(csv_path,\"PPMF20_0_UNIT.csv.gz\")}')\n",
    "WHERE VACS = 0\n",
    "GROUP BY TABBLKST, TABBLKCOU, TABTRACTCE, TABBLK\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "con.execute(initial_query)\n",
    "print(\"Created baseline housing table using PPMF0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define a function to process each iteration file, calculate COUNT, and calculate the differences\n",
    "def process_iteration_file_hu(iteration_file, iteration_number):\n",
    "    print(iteration_file)\n",
    "    \n",
    "    # Step: Drop the temporary table if it exists\n",
    "    drop_query = f\"DROP TABLE IF EXISTS iteration_{iteration_number}_hu\"\n",
    "\n",
    "    # Execute the query to drop the table\n",
    "    con.execute(drop_query)\n",
    "    \n",
    "    # Step 2a: Read the iteration file and calculate COUNT, then join it with the baseline_table\n",
    "    query = f\"\"\"\n",
    "    CREATE TEMPORARY TABLE iteration_{iteration_number}_hu AS\n",
    "    SELECT \n",
    "        CONCAT(b.TABBLKST, b.TABBLKCOU, b.TABTRACTCE, b.TABBLK) AS GEOID20,\n",
    "        COALESCE(a.PPMF0_COUNT, 0) AS PPMF0_COUNT,\n",
    "        COUNT(*) AS COUNT,\n",
    "        {iteration_number} as ITERATION,\n",
    "        (COUNT(*) - COALESCE(a.PPMF0_COUNT, 0)) AS DIFF,\n",
    "        ABS(COUNT(*) - COALESCE(a.PPMF0_COUNT, 0)) AS ABS_DIFF,\n",
    "        POW((COUNT(*) - COALESCE(a.PPMF0_COUNT, 0)), 2) AS SQRD_ERROR\n",
    "    FROM (\n",
    "        SELECT *\n",
    "        FROM read_csv_auto('{iteration_file}')\n",
    "        WHERE VACS = 0\n",
    "    ) b\n",
    "    LEFT JOIN baseline_table_hu a\n",
    "    ON a.GEOID20 = CONCAT(b.TABBLKST, b.TABBLKCOU, b.TABTRACTCE, b.TABBLK) \n",
    "    GROUP BY b.TABBLKST, b.TABBLKCOU, b.TABTRACTCE, b.TABBLK, a.PPMF0_COUNT, ITERATION\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the join and calculation query\n",
    "    con.execute(query)\n",
    "\n",
    "    # Step 2b: Insert the calculated results into the final result table\n",
    "    insert_query = f\"\"\"\n",
    "    INSERT INTO result_table_hu (GEOID20, PPMF0_COUNT, ITERATION, COUNT, DIFF, ABS_DIFF, SQRD_ERROR)\n",
    "    SELECT GEOID20, PPMF0_COUNT, ITERATION, COUNT, DIFF, ABS_DIFF, SQRD_ERROR FROM iteration_{iteration_number}_hu\n",
    "    \"\"\"\n",
    "    \n",
    "    con.execute(insert_query)\n",
    "\n",
    "    # Step: Drop the temporary table if it exists\n",
    "    drop_query = f\"DROP TABLE IF EXISTS iteration_{iteration_number}_hu\"\n",
    "\n",
    "    # Execute the query to drop the table\n",
    "    con.execute(drop_query)\n",
    "\n",
    "# Step 3: Create a final result table to store all results\n",
    "con.execute(\"DROP TABLE IF EXISTS result_table_hu\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE result_table_hu (\n",
    "    GEOID20 VARCHAR, \n",
    "    PPMF0_COUNT BIGINT, \n",
    "    ITERATION INT, \n",
    "    COUNT BIGINT, \n",
    "    DIFF BIGINT, \n",
    "    ABS_DIFF BIGINT, \n",
    "    SQRD_ERROR DOUBLE\n",
    ")\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Loop through the iteration files\n",
    "gz_csvs_folder = os.path.join(data_path, \"gz_csvs\")\n",
    "\n",
    "iteration_files = [os.path.join(gz_csvs_folder, f\"PPMF20_{i}_UNIT.csv.gz\") for i in range(1,51)]\n",
    "\n",
    "# Process each iteration file and calculate differences\n",
    "for i, iteration_file in enumerate(iteration_files, start=1):\n",
    "    process_iteration_file_hu(iteration_file, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step: Group by GEOID20 and calculate the statistics (MEAN, MSE, STD)\n",
    "query = f\"\"\"\n",
    "CREATE TABLE summary_stats_hu AS\n",
    "SELECT \n",
    "    GEOID20,\n",
    "    PPMF0_COUNT,\n",
    "    AVG(COUNT) AS MEAN,\n",
    "    AVG(SQRD_ERROR) AS MSE,\n",
    "    STDDEV(COUNT) AS STD\n",
    "-- FROM result_table_hu  \n",
    "FROM read_parquet('{data_path}/parquet/result_table_hu.parquet')\n",
    "GROUP BY GEOID20, PPMF0_COUNT;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query to create the summary statistics table\n",
    "con.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "ci_level = 0.9\n",
    "degrees_freedom = 5\n",
    "\n",
    "# Step 1: Set the t-value\n",
    "t_value = scipy.stats.t.ppf(q=1-(1-ci_level)/2, df=degrees_freedom) \n",
    "t_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Add intermediate calculations\n",
    "con.execute(\"DROP TABLE IF EXISTS intermediate_hu\")\n",
    "\n",
    "intermediate_query = f\"\"\"\n",
    "CREATE TEMPORARY TABLE intermediate_hu AS\n",
    "SELECT \n",
    "    *,\n",
    "    POWER(MSE, 0.5) AS RMSE,  \n",
    "    (MEAN - PPMF0_COUNT) AS BIAS,  \n",
    "    CASE \n",
    "        WHEN PPMF0_COUNT > 5 \n",
    "             AND ABS(MEAN - PPMF0_COUNT) / NULLIF(STD, 0) >= 0.5 \n",
    "             AND ((MEAN - PPMF0_COUNT) < 0 OR PPMF0_COUNT >= 25)\n",
    "        THEN TRUE \n",
    "        ELSE FALSE \n",
    "    END AS MET_CRITERIA, \n",
    "    CASE \n",
    "        WHEN PPMF0_COUNT > 5 \n",
    "             AND ABS(MEAN - PPMF0_COUNT) / NULLIF(STD, 0) >= 0.5 \n",
    "             AND ((MEAN - PPMF0_COUNT) < 0 OR PPMF0_COUNT >= 25)\n",
    "        THEN PPMF0_COUNT - (MEAN - PPMF0_COUNT) \n",
    "        ELSE PPMF0_COUNT \n",
    "    END AS POINT_EST\n",
    "-- FROM summary_stats_hu;\n",
    "FROM read_parquet('{data_path}/parquet/summary_stats_table_hu.parquet')\n",
    "\"\"\"\n",
    "con.execute(intermediate_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Calculate confidence intervals and create the final table\n",
    "con.execute(\"DROP TABLE IF EXISTS result_with_ci_hu\")\n",
    "\n",
    "final_query = f\"\"\"\n",
    "CREATE TABLE result_with_ci_hu AS\n",
    "SELECT \n",
    "    *,\n",
    "    GREATEST(FLOOR(POINT_EST - {t_value} * RMSE), 0) AS CI_LOW_HU, \n",
    "    CEIL(POINT_EST + {t_value} * RMSE) AS CI_HIGH_HU\n",
    "FROM intermediate_hu;\n",
    "\"\"\"\n",
    "con.execute(final_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file_hu = os.path.join(data_path, \"parquet/result_table_hu.parquet\")\n",
    "\n",
    "# Write the results_table_hu table to a Parquet file\n",
    "con.execute(f\"\"\"\n",
    "    COPY result_table_hu TO '{result_file_hu}' (FORMAT PARQUET)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "baseline_file_hu = os.path.join(data_path, \"parquet/baseline_table_hu.parquet\"\n",
    "\n",
    "# Write the baseline_table_hu table to a Parquet file\n",
    "con.execute(f\"\"\"\n",
    "    COPY baseline_table_hu TO '{baseline_file_hu}' (FORMAT PARQUET)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summary_stats_file_hu = os.path.join(data_path, \"parquet/summary_stats_table_hu.parquet\")\n",
    "\n",
    "# Write the summary_stats_hus table to a Parquet file\n",
    "con.execute(f\"\"\"\n",
    "    COPY summary_stats_hu TO '{summary_stats_file_hu}' (FORMAT PARQUET)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results_file_hu = os.path.join(data_path, \"parquet/final_results_hu.parquet\")\n",
    "\n",
    "# Step 2: Write the table to a Parquet file\n",
    "con.execute(f\"\"\"\n",
    "    COPY result_with_ci_hu TO '{final_results_file_hu}' (FORMAT PARQUET)\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
